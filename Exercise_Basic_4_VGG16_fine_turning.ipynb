{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning the top layers of the pre-trained VGG16\n",
    "We can try to \"fine-tune\" the last convolutional block of the VGG16 model alongside the top-level classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** Although Keras and tf.keras in most cases are compatible, but sometimes you will have unexpected errors. Most of the suggestions on the website are for Keras, some of them will not work for tf.keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "<keras.engine.input_layer.InputLayer object at 0x7f25ec52fe10> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f255ff7eb38> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f255ff7e9b0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f255ffab400> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f255ffab978> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f255f768160> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f255f70a2e8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f255f70acc0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f255f6be0f0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f255f6de4e0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f255f67f2e8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f255f67fcc0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f256b9090b8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f255f6530b8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f255f6727b8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f255f672630> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f255f62d358> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f255f5cb208> True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f255f5e8860> True\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 7,079,424\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "\n",
    "# path to the model weights files.\n",
    "weights_path = 'vgg16_weights_notop.h5'\n",
    "top_model_weights_path = 'fc_model.h5'\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/dogs-vs-cats-small/train'\n",
    "validation_data_dir = 'data/dogs-vs-cats-small/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "\n",
    "img_input_shape = (img_width,img_height,3)\n",
    "\n",
    "# build the VGG16 network\n",
    "vgg_conv = applications.VGG16(weights='imagenet', include_top=False,input_shape=img_input_shape)\n",
    "print('Model loaded.')\n",
    "\n",
    "# set the first 21 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)\n",
    "          \n",
    "vgg_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 9,177,089\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "\n",
    "total_model = Sequential()\n",
    "total_model.add(vgg_conv)\n",
    "total_model.add(Flatten())\n",
    "total_model.add(Dense(256, activation='relu'))\n",
    "total_model.add(Dropout(0.5))\n",
    "total_model.add(Dense(1, activation='sigmoid'))\n",
    "total_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyealy/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/lyealy/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=5, validation_data=<keras_pre..., steps_per_epoch=125, validation_steps=800)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "125/125 [==============================] - 41s 326ms/step - loss: 0.6133 - acc: 0.6675 - val_loss: 0.4192 - val_acc: 0.8462\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - 40s 317ms/step - loss: 0.4148 - acc: 0.8205 - val_loss: 0.3141 - val_acc: 0.8700\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - 39s 312ms/step - loss: 0.3358 - acc: 0.8580 - val_loss: 0.2618 - val_acc: 0.8912\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - 40s 318ms/step - loss: 0.2941 - acc: 0.8795 - val_loss: 0.2477 - val_acc: 0.9000\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - 39s 312ms/step - loss: 0.2578 - acc: 0.8925 - val_loss: 0.2162 - val_acc: 0.9087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f255f43e438>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "total_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "# fine-tune the model\n",
    "total_model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=nb_train_samples,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
