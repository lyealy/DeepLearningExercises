{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning the top layers of the pre-trained VGG16\n",
    "We can try to \"fine-tune\" the last convolutional block of the VGG16 model alongside the top-level classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** Although Keras and tf.keras in most cases are compatible, but sometimes you will have unexpected errors. Most of the suggestions on the website are for Keras, some of them will not work for tf.keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "<keras.engine.input_layer.InputLayer object at 0x7f422ca84eb8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f422ca849b0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f422ca84da0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f422ca0d5c0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f422ca0dc50> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f422e9666d8> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f422e901240> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f422e901f28> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f422e8bb240> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f422e8d6518> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f422e875470> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f422e875f28> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f422e833240> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f422e84d518> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f422e86d470> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f422e86df28> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f422e82e240> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x7f422e7c7518> True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x7f422e7e7470> True\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 7,079,424\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "\n",
    "# path to the model weights files.\n",
    "weights_path = 'vgg16_weights_notop.h5'\n",
    "top_model_weights_path = 'fc_model.h5'\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/dogs-vs-cats-small/train'\n",
    "validation_data_dir = 'data/dogs-vs-cats-small/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "\n",
    "img_input_shape = (img_width,img_height,3)\n",
    "\n",
    "# build the VGG16 network\n",
    "vgg_conv = applications.VGG16(weights='imagenet', include_top=False,input_shape=img_input_shape)\n",
    "print('Model loaded.')\n",
    "\n",
    "# set the first 21 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)\n",
    "          \n",
    "vgg_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 9,177,089\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "\n",
    "total_model = Sequential()\n",
    "total_model.add(vgg_conv)\n",
    "total_model.add(Flatten())\n",
    "total_model.add(Dense(256, activation='relu'))\n",
    "total_model.add(Dropout(0.5))\n",
    "total_model.add(Dense(1, activation='sigmoid'))\n",
    "total_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyealy/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/home/lyealy/anaconda3/envs/tf/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., epochs=5, validation_data=<keras_pre..., steps_per_epoch=125, validation_steps=800)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "125/125 [==============================] - 40s 324ms/step - loss: 0.6541 - acc: 0.6400 - val_loss: 0.4800 - val_acc: 0.8050\n",
      "Epoch 2/5\n",
      "125/125 [==============================] - 40s 320ms/step - loss: 0.4453 - acc: 0.7990 - val_loss: 0.3386 - val_acc: 0.8675\n",
      "Epoch 3/5\n",
      "125/125 [==============================] - 41s 325ms/step - loss: 0.3696 - acc: 0.8375 - val_loss: 0.2778 - val_acc: 0.8912\n",
      "Epoch 4/5\n",
      "125/125 [==============================] - 40s 317ms/step - loss: 0.3003 - acc: 0.8635 - val_loss: 0.2619 - val_acc: 0.8912\n",
      "Epoch 5/5\n",
      "125/125 [==============================] - 40s 319ms/step - loss: 0.2664 - acc: 0.8935 - val_loss: 0.2223 - val_acc: 0.9062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f422e578320>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "total_model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "# fine-tune the model\n",
    "total_model.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=nb_train_samples,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
